@InProceedings{10.1007/978-3-031-15934-3_67,
author="Yorsh, Uladzislau
and Kovalenko, Alexander",
editor="Pimenidis, Elias
and Angelov, Plamen
and Jayne, Chrisina
and Papaleonidas, Antonios
and Aydin, Mehmet",
title="Linear Self-attention Approximation viaÂ Trainable Feedforward Kernel",
booktitle="Artificial Neural Networks and Machine Learning -- ICANN 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="807--810",
abstract="Restrictive limitation of Transformers [18] due to the quadratic complexity of self-attention mechanism motivated a new research field of efficient Transformers [17], which approximate the original architecture with asymptotically faster models.",
isbn="978-3-031-15934-3"
}

