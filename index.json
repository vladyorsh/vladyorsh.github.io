[{"authors":["admin"],"categories":null,"content":"","date":1715385600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1715385600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://vladyorsh.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"","tags":null,"title":"Uladzislau Yorsh","type":"authors"},{"authors":["Uladzislau Yorsh","Martin Holeňa","Ondřej Bojar","David Herel"],"categories":null,"content":" ","date":1715385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1715385600,"objectID":"000a5cbb9f9e706c8aaa6c983193f267","permalink":"https://vladyorsh.github.io/publication/shared_mem/","publishdate":"2024-05-11T00:00:00Z","relpermalink":"/publication/shared_mem/","section":"publication","summary":"Our findings challenge the conventional thinking on the models that use external learnable memory, like Luna or Memory Augmented Transformer, to reduce the computational complexity. We reveal that interfacing with the memory directly through an attention operation is suboptimal, and that the models' performance may be considerably improved by filtering the input signal before communicating with it.","tags":null,"title":"On Difficulties of Attention Factorization through Shared Memory","type":"publication"},{"authors":["Uladzislau Yorsh","Alexander S. Behr","Norbert Kockmann","Martin Holeňa"],"categories":null,"content":" ","date":1663632000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663632000,"objectID":"2f93a70e07b619c51223b1b984a1c7ec","permalink":"https://vladyorsh.github.io/publication/onto_mapping/","publishdate":"2022-09-20T00:00:00Z","relpermalink":"/publication/onto_mapping/","section":"publication","summary":"We have started to automatize the process of an automatic assignment of a relevant ontology to an input article, and attack the problem by utilizing state-of-the-art NLP tools and neural networks. We assess the quality by visializing the latent space of annotation and text embeddings and sampling examples of mappings between text fragments and ontology annotations.","tags":null,"title":"Text-to-Ontology Mapping via Natural Language Processing Models","type":"publication"},{"authors":["Uladzislau Yorsh","Alexander Kovalenko"],"categories":null,"content":" ","date":1662681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662681600,"objectID":"fd29a1c124f3b519cc6114cd26bff7ee","permalink":"https://vladyorsh.github.io/publication/feedforward_kernel/","publishdate":"2022-09-09T00:00:00Z","relpermalink":"/publication/feedforward_kernel/","section":"publication","summary":"We employ the kernelized formulation of an attention computation in Transformer, and evaluate the kernel implementation as FFNN on the subset of the Long Range Arena.","tags":null,"title":"Linear Self-Attention Approximation via Trainable Feedforward Kernel","type":"publication"},{"authors":["Uladzislau Yorsh","Alexander Kovalenko","Vojtěch Vančura","Daniel Vašata","Pavel Kordík","Tomáš Mikolov"],"categories":null,"content":" ","date":1656374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656374400,"objectID":"7227346412123d79985f18a66055d2a7","permalink":"https://vladyorsh.github.io/publication/simpletron/","publishdate":"2022-06-28T00:00:00Z","relpermalink":"/publication/simpletron/","section":"publication","summary":"We have suggested, that a Transformer attention module can be implemented without a nonlinearity between query and key multiplication, and evaluated our findings on the Long Range Arena subset.","tags":null,"title":"SimpleTRON: Simple Transformer with O(N) Complexity","type":"publication"},{"authors":null,"categories":null,"content":"\u0026hellip;\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"https://vladyorsh.github.io/privacy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/privacy/","section":"","summary":"\u0026hellip;","tags":null,"title":"","type":"page"}]