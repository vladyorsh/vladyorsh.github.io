[{"authors":["admin"],"categories":null,"content":"","date":1663891200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1663891200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://vladyorsh.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"","tags":null,"title":"Uladzislau Yorsh","type":"authors"},{"authors":["Uladzislau Yorsh","Alexander S. Behr","Norbert Kockmann","Martin Holeňa"],"categories":null,"content":"","date":1663891200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663891200,"objectID":"2f93a70e07b619c51223b1b984a1c7ec","permalink":"https://vladyorsh.github.io/publication/onto_mapping/","publishdate":"2022-09-30T00:00:00Z","relpermalink":"/publication/onto_mapping/","section":"publication","summary":"We have started to automatize the process of an automatic assignment of a relevant ontology to an input article, and attack the problem by utilizing state-of-the-art NLP tools and neural networks. We assess the quality by visializing the latent space of annotation and text embeddings and sampling examples of mappings between text fragments and ontology annotations.","tags":null,"title":"Text-to-Ontology Mapping via Natural Language Processing Models","type":"publication"},{"authors":["Uladzislau Yorsh","Alexander Kovalenko"],"categories":null,"content":"","date":1662681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662681600,"objectID":"fd29a1c124f3b519cc6114cd26bff7ee","permalink":"https://vladyorsh.github.io/publication/feedforward_kernel/","publishdate":"2022-09-09T00:00:00Z","relpermalink":"/publication/feedforward_kernel/","section":"publication","summary":"We employ the kernelized formulation of an attention computation in Transformer, and evaluate the kernel implementation as FFNN on the subset of the Long Range Arena.","tags":null,"title":"Linear Self-Attention Approximation via Trainable Feedforward Kernel","type":"publication"},{"authors":["Uladzislau Yorsh","Alexander Kovalenko","Vojtěch Vančura","Daniel Vašata","Pavel Kordík","Tomáš Mikolov"],"categories":null,"content":"","date":1656374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656374400,"objectID":"7227346412123d79985f18a66055d2a7","permalink":"https://vladyorsh.github.io/publication/simpletron/","publishdate":"2022-06-28T00:00:00Z","relpermalink":"/publication/simpletron/","section":"publication","summary":"We have suggested, that a Transformer attention module can be implemented without a nonlinearity between query and key multiplication, and evaluated our findings on the Long Range Arena subset.","tags":null,"title":"SimpleTRON: Simple Transformer with O(N) Complexity","type":"publication"},{"authors":null,"categories":null,"content":"#\u0026mdash; #date: \u0026ldquo;2018-06-28T00:00:00+01:00\u0026rdquo; #draft: true #header:\ncaption: \u0026quot;\u0026quot; image: \u0026quot;\u0026quot; #share: false #title: Privacy Policy #\u0026mdash;\n#\u0026hellip;\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"https://vladyorsh.github.io/privacy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/privacy/","section":"","summary":"#\u0026mdash; #date: \u0026ldquo;2018-06-28T00:00:00+01:00\u0026rdquo; #draft: true #header:\ncaption: \u0026quot;\u0026quot; image: \u0026quot;\u0026quot; #share: false #title: Privacy Policy #\u0026mdash;\n#\u0026hellip;","tags":null,"title":"","type":"page"}]